{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from mediapipe.python.solutions import pose as mp_pose\n",
    "from mediapipe.python.solutions import drawing_utils as mp_drawing\n",
    "import tqdm\n",
    "import sys\n",
    "import cv2\n",
    "import requests\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose_classification_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIiEj8Tx_x-q"
   },
   "source": [
    "# Step 1: Build classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpRszzilECFz"
   },
   "source": [
    "## Upload image samples\n",
    "\n",
    "Locally create a folder named `fitness_poses_images_in` with image samples.\n",
    "\n",
    "Images should repesent terminal states of desired pose classes. I.e. if you want to classify push-up provide iamges for two classes: when person is up, and when person is down.\n",
    "\n",
    "There should be about a few hundred samples per class covering different camera angles, environment conditions, body shapes, and exercise variations to build a good classifier.\n",
    "\n",
    "Required structure of the images_in_folder:\n",
    "```\n",
    "fitness_poses_images_in/\n",
    "  pushups_up/\n",
    "    image_001.jpg\n",
    "    image_002.jpg\n",
    "    ...\n",
    "  pushups_down/\n",
    "    image_001.jpg\n",
    "    image_002.jpg\n",
    "    ...\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QBS_P_Y_2mg"
   },
   "source": [
    "## Bootstrap images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bERVPO8Ja6j7"
   },
   "outputs": [],
   "source": [
    "bootstrap_images_in_folder = 'fitness_data/raw_train/squats'\n",
    "\n",
    "# Output folders for bootstrapped images and CSVs.\n",
    "bootstrap_images_out_folder = 'fitness_data/processed_train/squats/images_out'\n",
    "bootstrap_csvs_out_folder = 'fitness_data/processed_train/squats/csvs_out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PVYsbbJbOW7W"
   },
   "outputs": [],
   "source": [
    "# Initialize helper.\n",
    "bootstrap_helper = BootstrapHelper(\n",
    "    images_in_folder=bootstrap_images_in_folder,\n",
    "    images_out_folder=bootstrap_images_out_folder,\n",
    "    csvs_out_folder=bootstrap_csvs_out_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e832H-X6b-v7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images per pose class:\n",
      "  squats_down: 100\n",
      "  squats_up: 100\n"
     ]
    }
   ],
   "source": [
    "# Check how many pose classes and images for them are available.\n",
    "bootstrap_helper.print_images_in_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EAWtcZSHcQHc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrapping  squats_down\n",
      "100%|██████████| 100/100 [00:34<00:00,  2.93it/s]\n",
      "Bootstrapping  squats_up\n",
      "100%|██████████| 100/100 [00:32<00:00,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap all images.\n",
    "# Set limit to some small number for debug.\n",
    "bootstrap_helper.bootstrap(per_pose_class_limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xRdqXeUScko9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images per pose class:\n",
      "  pushups_down: 100\n",
      "  pushups_up: 100\n"
     ]
    }
   ],
   "source": [
    "# Check how many images were bootstrapped.\n",
    "bootstrap_helper.print_images_out_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rTc3dlvFgg50"
   },
   "outputs": [],
   "source": [
    "# After initial bootstrapping images without detected poses were still saved in\n",
    "# the folderd (but not in the CSVs) for debug purpose. Let's remove them.\n",
    "# bootstrap_helper.align_images_and_csvs(print_removed_items=False)\n",
    "# bootstrap_helper.print_images_out_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3Sjl9JcJQzN"
   },
   "source": [
    "## Manual filtration\n",
    "\n",
    "Please manually verify predictions and remove samples (images) that has wrong pose prediction. Check as if you were asked to classify pose just from predicted landmarks. If you can't - remove it.\n",
    "\n",
    "Align CSVs and image folders once you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fI7OytDZiHmg"
   },
   "outputs": [],
   "source": [
    "# Align CSVs with filtered images.\n",
    "bootstrap_helper.align_images_and_csvs(print_removed_items=False)\n",
    "bootstrap_helper.print_images_out_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGxcMoDLK8L0"
   },
   "source": [
    "## Automatic filtration\n",
    "\n",
    "Classify each sample against database of all other samples and check if it gets in the same class as annotated after classification.\n",
    "\n",
    "There can be two reasons for the outliers:\n",
    "\n",
    "  * **Wrong pose prediction**: In this case remove such outliers.\n",
    "\n",
    "  * **Wrong classification** (i.e. pose is predicted correctly and you aggree with original pose class assigned to the sample): In this case sample is from the underrepresented group (e.g. unusual angle or just very few samples). Add more similar samples and run bootstrapping from the very beginning.\n",
    "\n",
    "Even if you just removed some samples it makes sence to re-run automatic filtration one more time as database of poses has changed.\n",
    "\n",
    "**Important!!** Check that you are using the same parameters when classifying whole videos later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txnkReCiJr4Y"
   },
   "outputs": [],
   "source": [
    "# Find outliers.\n",
    "\n",
    "# Transforms pose landmarks into embedding.\n",
    "pose_embedder = FullBodyPoseEmbedder()\n",
    "\n",
    "# Classifies given pose against database of poses.\n",
    "pose_classifier = PoseClassifier(\n",
    "    pose_samples_folder=bootstrap_csvs_out_folder,\n",
    "    pose_embedder=pose_embedder,\n",
    "    top_n_by_max_distance=30,\n",
    "    top_n_by_mean_distance=10)\n",
    "\n",
    "outliers = pose_classifier.find_pose_sample_outliers()\n",
    "print('Number of outliers: ', len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HS6TkprSklIw"
   },
   "outputs": [],
   "source": [
    "# Analyze outliers.\n",
    "bootstrap_helper.analyze_outliers(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTWgo8-rlDFv"
   },
   "outputs": [],
   "source": [
    "# Remove all outliers (if you don't want to manually pick).\n",
    "bootstrap_helper.remove_outliers(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LR9xOhPdlrkN"
   },
   "outputs": [],
   "source": [
    "# Align CSVs with images after removing outliers.\n",
    "bootstrap_helper.align_images_and_csvs(print_removed_items=False)\n",
    "bootstrap_helper.print_images_out_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nd0OSLWXMJFC"
   },
   "source": [
    "## Dump for the App\n",
    "\n",
    "Dump filtered poses to CSV and download it.\n",
    "\n",
    "Please check this [guide](https://developers.google.com/ml-kit/vision/pose-detection/classifying-poses#4_integrate_with_the_ml_kit_quickstart_app) on how to use this CSV in the ML Kit sample app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-2VfIiLPML8A"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def dump_for_the_app():\n",
    "  pose_samples_folder = 'fitness_poses_csvs_out'\n",
    "  pose_samples_csv_path = 'fitness_poses_csvs_out.csv'\n",
    "  file_extension = 'csv'\n",
    "  file_separator = ','\n",
    "\n",
    "  # Each file in the folder represents one pose class.\n",
    "  file_names = [name for name in os.listdir(pose_samples_folder) if name.endswith(file_extension)]\n",
    "\n",
    "  with open(pose_samples_csv_path, 'w') as csv_out:\n",
    "    csv_out_writer = csv.writer(csv_out, delimiter=file_separator, quoting=csv.QUOTE_MINIMAL)\n",
    "    for file_name in file_names:\n",
    "      # Use file name as pose class name.\n",
    "      class_name = file_name[:-(len(file_extension) + 1)]\n",
    "\n",
    "      # One file line: `sample_00001,x1,y1,x2,y2,....`.\n",
    "      with open(os.path.join(pose_samples_folder, file_name)) as csv_in:\n",
    "        csv_in_reader = csv.reader(csv_in, delimiter=file_separator)\n",
    "        for row in csv_in_reader:\n",
    "          row.insert(1, class_name)\n",
    "          csv_out_writer.writerow(row)\n",
    "\n",
    "\n",
    "dump_for_the_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfE7C9wHel_7"
   },
   "source": [
    "# Step 2: Classification\n",
    "\n",
    "**Important!!** Check that you are using the same classification parameters as while building classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYkZJ3a_2MW_"
   },
   "outputs": [],
   "source": [
    "# Specify your video name and target pose class to count the repetitions.\n",
    "video_path = 'pushups-sample.mov'\n",
    "class_name='pushups_down'\n",
    "out_video_path = 'pushups-sample-out.mov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3gGMDE0R2pe"
   },
   "outputs": [],
   "source": [
    "# Open the video.\n",
    "import cv2\n",
    "\n",
    "video_cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get some video parameters to generate output video with classificaiton.\n",
    "video_n_frames = video_cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "video_fps = video_cap.get(cv2.CAP_PROP_FPS)\n",
    "video_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = cv2.imread('test1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with pose class CSVs. That should be the same folder you using while\n",
    "# building classifier to output CSVs.\n",
    "pose_samples_folder = 'fitness_data/processed_train/pushups/csvs_out'\n",
    "\n",
    "# Initialize tracker.\n",
    "pose_tracker = mp_pose.Pose()\n",
    "\n",
    "# Initialize embedder.\n",
    "pose_embedder = FullBodyPoseEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_classifier = PoseClassifier(\n",
    "    pose_samples_folder=pose_samples_folder,\n",
    "    pose_embedder=pose_embedder,\n",
    "    top_n_by_max_distance=30,\n",
    "    top_n_by_mean_distance=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pose tracker\n",
    "input_frame = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "result = pose_tracker.process(image=input_frame)\n",
    "pose_landmarks = result.pose_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_frame = input_frame.copy()\n",
    "if pose_landmarks is not None:\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=output_frame,\n",
    "        landmark_list=pose_landmarks,\n",
    "        connections=mp_pose.POSE_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get landmarks.\n",
    "frame_height, frame_width = output_frame.shape[0], output_frame.shape[1]\n",
    "pose_landmarks = np.array([[lmk.x * frame_width, lmk.y * frame_height, lmk.z * frame_width]\n",
    "                            for lmk in pose_landmarks.landmark], dtype=np.float32)\n",
    "assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pushups_up': 10}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose_classifier(pose_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7t_ACEmTSOhr"
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Uncomment to validate target poses used by classifier and find outliers.\n",
    "# outliers = pose_classifier.find_pose_sample_outliers()\n",
    "# print('Number of pose sample outliers (consider removing them): ', len(outliers))\n",
    "\n",
    "# Initialize EMA smoothing.\n",
    "pose_classification_filter = EMADictSmoothing(\n",
    "    window_size=10,\n",
    "    alpha=0.2)\n",
    "\n",
    "# Initialize counter.\n",
    "repetition_counter = RepetitionCounter(\n",
    "    class_name=class_name,\n",
    "    enter_threshold=6,\n",
    "    exit_threshold=4)\n",
    "\n",
    "# Initialize renderer.\n",
    "pose_classification_visualizer = PoseClassificationVisualizer(\n",
    "    class_name=class_name,\n",
    "    plot_x_max=video_n_frames,\n",
    "    # Graphic looks nicer if it's the same as `top_n_by_mean_distance`.\n",
    "    plot_y_max=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lXymkneOjgZ"
   },
   "outputs": [],
   "source": [
    "# Run classification on a video.\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from mediapipe.python.solutions import drawing_utils as mp_drawing\n",
    "\n",
    "\n",
    "# Open output video.\n",
    "out_video = cv2.VideoWriter(out_video_path, cv2.VideoWriter_fourcc(*'mp4v'), video_fps, (video_width, video_height))\n",
    "\n",
    "frame_idx = 0\n",
    "output_frame = None\n",
    "with tqdm.tqdm(total=video_n_frames, position=0, leave=True) as pbar:\n",
    "  while True:\n",
    "    # Get next frame of the video.\n",
    "    success, input_frame = video_cap.read()\n",
    "    if not success:\n",
    "      break\n",
    "\n",
    "    # Run pose tracker.\n",
    "    input_frame = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)\n",
    "    result = pose_tracker.process(image=input_frame)\n",
    "    pose_landmarks = result.pose_landmarks\n",
    "\n",
    "    # Draw pose prediction.\n",
    "    output_frame = input_frame.copy()\n",
    "    if pose_landmarks is not None:\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=output_frame,\n",
    "          landmark_list=pose_landmarks,\n",
    "          connections=mp_pose.POSE_CONNECTIONS)\n",
    "    \n",
    "    if pose_landmarks is not None:\n",
    "      # Get landmarks.\n",
    "      frame_height, frame_width = output_frame.shape[0], output_frame.shape[1]\n",
    "      pose_landmarks = np.array([[lmk.x * frame_width, lmk.y * frame_height, lmk.z * frame_width]\n",
    "                                 for lmk in pose_landmarks.landmark], dtype=np.float32)\n",
    "      assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)\n",
    "\n",
    "      # Classify the pose on the current frame.\n",
    "      pose_classification = pose_classifier(pose_landmarks)\n",
    "\n",
    "      # Smooth classification using EMA.\n",
    "      pose_classification_filtered = pose_classification_filter(pose_classification)\n",
    "\n",
    "      # Count repetitions.\n",
    "      repetitions_count = repetition_counter(pose_classification_filtered)\n",
    "    else:\n",
    "      # No pose => no classification on current frame.\n",
    "      pose_classification = None\n",
    "\n",
    "      # Still add empty classification to the filter to maintaing correct\n",
    "      # smoothing for future frames.\n",
    "      pose_classification_filtered = pose_classification_filter(dict())\n",
    "      pose_classification_filtered = None\n",
    "\n",
    "      # Don't update the counter presuming that person is 'frozen'. Just\n",
    "      # take the latest repetitions count.\n",
    "      repetitions_count = repetition_counter.n_repeats\n",
    "\n",
    "    # Draw classification plot and repetition counter.\n",
    "    output_frame = pose_classification_visualizer(\n",
    "        frame=output_frame,\n",
    "        pose_classification=pose_classification,\n",
    "        pose_classification_filtered=pose_classification_filtered,\n",
    "        repetitions_count=repetitions_count)\n",
    "\n",
    "    # Save the output frame.\n",
    "    out_video.write(cv2.cvtColor(np.array(output_frame), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Show intermediate frames of the video to track progress.\n",
    "    if frame_idx % 50 == 0:\n",
    "      show_image(output_frame)\n",
    "\n",
    "    frame_idx += 1\n",
    "    pbar.update()\n",
    "\n",
    "# Close output video.\n",
    "out_video.release()\n",
    "\n",
    "# Release MediaPipe resources.\n",
    "pose_tracker.close()\n",
    "\n",
    "# Show the last frame of the video.\n",
    "if output_frame is not None:\n",
    "  show_image(output_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJFftPiQE56E"
   },
   "outputs": [],
   "source": [
    "# Download generated video\n",
    "files.download(out_video_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pose classification (extended).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "06cf0ec498029f8561fed56a63f8d355b89e528f84c190c1f9869c0e1c0421ca"
  },
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
